**第一章：NPL（自然语言处理）基础知识**
让计算机理解、解释和生成人类语言的技术
RNN→LSTM→Attention→BERT→transformer(GPT-3)
任务：CWS（中文分词）、子次切分、词性标注、文本分类、实体识别、关系抽取、文本摘要、机器翻译、自动问答
词向量（vsm）：数据稀疏性和维度灾难
N-gram:基于统计马尔科夫链，一个词出现的概率依据前n-1个词概率
 ↓
Word2vec
 ↓
ELMo
**第二章：transformer架构**
最开始NLP专用处理时序的RNN和其衍生架构LSTM
 ↓
出现问题，因为RNN需要依次输入序列读入内存所以不能使用GPU的并行计算功能
《attention is all you need》：query查询、key键值、value真值
q由注意力发起人发起，表示注意什么如我现在注意力集中在颜色上面
k表示其他接受注意的，这里是整个文本，表示可以引起多少注意
v表k所对应的文本的本来的值，qk相乘表示注意力权重
(Attention(Q,K,V)) = softmax(QK^T/sqrt{d})V  (softmax转化其为和为1的权重，除以维度的开方减小向量的大维度对稳定性影响)

注意力机制的实现→自注意力机制（Q,K,V来自同一个序列），掩码自注意力